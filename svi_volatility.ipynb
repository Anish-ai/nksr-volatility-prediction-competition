{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_advanced_statistics(df):\n",
    "    \"\"\"Calculate advanced statistical metrics\"\"\"\n",
    "    print(\"Calculating advanced statistics...\")\n",
    "    stats = df.copy()\n",
    "\n",
    "    stats['underlying_returns'] = np.log(stats['underlying'] / stats['underlying'].shift(1))\n",
    "    \n",
    "    # Volatility of volatility\n",
    "    stats['vol_of_vol'] = stats['underlying_returns'].rolling(window=20).std().rolling(window=20).std()\n",
    "    \n",
    "    # Realized volatility\n",
    "    stats['realized_vol'] = stats['underlying_returns'].rolling(window=20).std() * np.sqrt(252)\n",
    "    \n",
    "    # Volatility skew\n",
    "    stats['vol_skew'] = stats['underlying_returns'].rolling(window=20).skew()\n",
    "    \n",
    "    # Volatility kurtosis\n",
    "    stats['vol_kurt'] = stats['underlying_returns'].rolling(window=20).kurt()\n",
    "    \n",
    "    # Volatility term structure\n",
    "    for window in [5, 10, 20, 60]:\n",
    "        stats[f'vol_term_{window}'] = stats['underlying_returns'].rolling(window=window).std() * np.sqrt(252)\n",
    "    \n",
    "    # Fill numerical values first\n",
    "    numerical_cols = stats.select_dtypes(include=[np.number]).columns\n",
    "    stats[numerical_cols] = stats[numerical_cols].fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "    \n",
    "    # Volatility regime indicators - create after filling numerical values\n",
    "    try:\n",
    "        stats['vol_regime'] = pd.qcut(stats['realized_vol'], q=5, labels=['very_low', 'low', 'medium', 'high', 'very_high'])\n",
    "    except ValueError:\n",
    "        # Handle case when there are not enough unique values\n",
    "        stats['vol_regime'] = pd.Series(['medium'] * len(stats), index=stats.index)\n",
    "    \n",
    "    # Fill any remaining categorical columns with their mode\n",
    "    categorical_cols = stats.select_dtypes(include=['category']).columns\n",
    "    for col in categorical_cols:\n",
    "        mode_val = stats[col].mode()[0] if not stats[col].mode().empty else None\n",
    "        stats[col] = stats[col].fillna(mode_val)\n",
    "    \n",
    "    print(f\"Created {len(stats.columns)} statistical features\")\n",
    "    return stats\n",
    "\n",
    "# Test statistics calculation on a small sample\n",
    "sample_stats = calculate_advanced_statistics(train.head(100))\n",
    "print(f\"\\nSample statistics shape: {sample_stats.shape}\")\n",
    "print(\"\\nStatistical features:\")\n",
    "print(sample_stats.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Train data shape: (178340, 97)\n",
      "Test data shape: (12065, 96)\n",
      "Sample submission shape: (12065, 53)\n",
      "\n",
      "Number of IV columns: 52\n",
      "\n",
      "Number of unique strikes: 36\n",
      "\n",
      "Strike dictionary:\n",
      "{'24000': {'call': 'call_iv_24000', 'put': 'put_iv_24000'}, '24100': {'call': 'call_iv_24100', 'put': 'put_iv_24100'}, '24200': {'call': 'call_iv_24200', 'put': 'put_iv_24200'}, '24300': {'call': 'call_iv_24300', 'put': 'put_iv_24300'}, '24400': {'call': 'call_iv_24400', 'put': 'put_iv_24400'}, '24500': {'call': 'call_iv_24500', 'put': 'put_iv_24500'}, '24600': {'call': 'call_iv_24600', 'put': 'put_iv_24600'}, '24700': {'call': 'call_iv_24700', 'put': 'put_iv_24700'}, '24800': {'call': 'call_iv_24800', 'put': 'put_iv_24800'}, '24900': {'call': 'call_iv_24900', 'put': 'put_iv_24900'}, '25000': {'call': 'call_iv_25000', 'put': 'put_iv_25000'}, '25100': {'call': 'call_iv_25100', 'put': 'put_iv_25100'}, '25200': {'call': 'call_iv_25200', 'put': 'put_iv_25200'}, '25300': {'call': 'call_iv_25300', 'put': 'put_iv_25300'}, '25400': {'call': 'call_iv_25400', 'put': 'put_iv_25400'}, '25500': {'call': 'call_iv_25500', 'put': 'put_iv_25500'}, '25600': {'call': 'call_iv_25600', 'put': None}, '25700': {'call': 'call_iv_25700', 'put': None}, '25800': {'call': 'call_iv_25800', 'put': None}, '25900': {'call': 'call_iv_25900', 'put': None}, '26000': {'call': 'call_iv_26000', 'put': None}, '26100': {'call': 'call_iv_26100', 'put': None}, '26200': {'call': 'call_iv_26200', 'put': None}, '26300': {'call': 'call_iv_26300', 'put': None}, '26400': {'call': 'call_iv_26400', 'put': None}, '26500': {'call': 'call_iv_26500', 'put': None}, '23000': {'call': None, 'put': 'put_iv_23000'}, '23100': {'call': None, 'put': 'put_iv_23100'}, '23200': {'call': None, 'put': 'put_iv_23200'}, '23300': {'call': None, 'put': 'put_iv_23300'}, '23400': {'call': None, 'put': 'put_iv_23400'}, '23500': {'call': None, 'put': 'put_iv_23500'}, '23600': {'call': None, 'put': 'put_iv_23600'}, '23700': {'call': None, 'put': 'put_iv_23700'}, '23800': {'call': None, 'put': 'put_iv_23800'}, '23900': {'call': None, 'put': 'put_iv_23900'}}\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "train = pd.read_parquet('train_data.parquet')\n",
    "test = pd.read_parquet('test_data.parquet')\n",
    "sample_sub = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "print(f\"\\nTrain data shape: {train.shape}\")\n",
    "print(f\"Test data shape: {test.shape}\")\n",
    "print(f\"Sample submission shape: {sample_sub.shape}\")\n",
    "\n",
    "# Get all IV columns from TEST data\n",
    "iv_columns = [col for col in test.columns if col.startswith(('call_iv_', 'put_iv_'))]\n",
    "print(f\"\\nNumber of IV columns: {len(iv_columns)}\")\n",
    "\n",
    "# Create strike dictionary from TEST columns\n",
    "strike_dict = {}\n",
    "for col in iv_columns:\n",
    "    strike = col.split('_')[-1]\n",
    "    if strike not in strike_dict:\n",
    "        strike_dict[strike] = {'call': None, 'put': None}\n",
    "    \n",
    "    if col.startswith('call_iv_'):\n",
    "        strike_dict[strike]['call'] = col\n",
    "    else:\n",
    "        strike_dict[strike]['put'] = col\n",
    "\n",
    "print(f\"\\nNumber of unique strikes: {len(strike_dict)}\")\n",
    "print(\"\\nStrike dictionary:\")\n",
    "print(strike_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SVI Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVI model functions defined successfully\n"
     ]
    }
   ],
   "source": [
    "def svi(k, a, b, rho, m, sigma):\n",
    "    \"\"\"SVI parametrization\"\"\"\n",
    "    return a + b * (rho * (k - m) + np.sqrt((k - m)**2 + sigma**2))\n",
    "\n",
    "def svi_objective(params, k, w):\n",
    "    \"\"\"Objective function for SVI fitting\"\"\"\n",
    "    a, b, rho, m, sigma = params\n",
    "    w_fit = svi(k, a, b, rho, m, sigma)\n",
    "    return np.sum((w - w_fit)**2)\n",
    "\n",
    "def fit_svi(k, w, initial_params=None):\n",
    "    \"\"\"Fit SVI model to data\"\"\"\n",
    "    if initial_params is None:\n",
    "        initial_params = [0.04, 0.4, 0.0, 0.0, 0.1]\n",
    "    \n",
    "    bounds = [\n",
    "        (-np.inf, np.inf),  # a\n",
    "        (0, np.inf),        # b\n",
    "        (-1, 1),            # rho\n",
    "        (-np.inf, np.inf),  # m\n",
    "        (0, np.inf)         # sigma\n",
    "    ]\n",
    "    \n",
    "    result = minimize(\n",
    "        svi_objective,\n",
    "        initial_params,\n",
    "        args=(k, w),\n",
    "        bounds=bounds,\n",
    "        method='L-BFGS-B'\n",
    "    )\n",
    "    \n",
    "    return result.x\n",
    "\n",
    "print(\"SVI model functions defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Statistical Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating advanced statistics...\n",
      "Warning: Could not create volatility regime categories: Bin edges must be unique: Index([0.0005179523781504376,  0.000991424787560999,  0.010145339590495671,\n",
      "        0.011149884145045185,  0.022417607704103816,  0.022417607704103816],\n",
      "      dtype='float64', name='realized_vol').\n",
      "You can drop duplicate edges by setting the 'duplicates' kwarg\n",
      "Created 107 statistical features\n",
      "\n",
      "Sample statistics shape: (100, 107)\n",
      "\n",
      "Statistical features:\n",
      "['timestamp', 'underlying', 'expiry', 'call_iv_23500', 'call_iv_23600', 'call_iv_23700', 'call_iv_23800', 'call_iv_23900', 'call_iv_24000', 'call_iv_24100', 'call_iv_24200', 'call_iv_24300', 'call_iv_24400', 'call_iv_24500', 'call_iv_24600', 'call_iv_24700', 'call_iv_24800', 'call_iv_24900', 'call_iv_25000', 'call_iv_25100', 'call_iv_25200', 'call_iv_25300', 'call_iv_25400', 'call_iv_25500', 'call_iv_25600', 'call_iv_25700', 'call_iv_25800', 'call_iv_25900', 'call_iv_26000', 'put_iv_22500', 'put_iv_22600', 'put_iv_22700', 'put_iv_22800', 'put_iv_22900', 'put_iv_23000', 'put_iv_23100', 'put_iv_23200', 'put_iv_23300', 'put_iv_23400', 'put_iv_23500', 'put_iv_23600', 'put_iv_23700', 'put_iv_23800', 'put_iv_23900', 'put_iv_24000', 'put_iv_24100', 'put_iv_24200', 'put_iv_24300', 'put_iv_24400', 'put_iv_24500', 'put_iv_24600', 'put_iv_24700', 'put_iv_24800', 'put_iv_24900', 'put_iv_25000', 'X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'X24', 'X25', 'X26', 'X27', 'X28', 'X29', 'X30', 'X31', 'X32', 'X33', 'X34', 'X35', 'X36', 'X37', 'X38', 'X39', 'X40', 'X41', 'underlying_returns', 'vol_of_vol', 'realized_vol', 'vol_skew', 'vol_kurt', 'vol_term_5', 'vol_term_10', 'vol_term_20', 'vol_term_60', 'vol_regime']\n"
     ]
    }
   ],
   "source": [
    "def calculate_advanced_statistics(df):\n",
    "    \"\"\"Calculate advanced statistical metrics\"\"\"\n",
    "    print(\"Calculating advanced statistics...\")\n",
    "    stats = df.copy()\n",
    "\n",
    "    stats['underlying_returns'] = np.log(stats['underlying'] / stats['underlying'].shift(1))\n",
    "    \n",
    "    # Volatility of volatility\n",
    "    stats['vol_of_vol'] = stats['underlying_returns'].rolling(window=20).std().rolling(window=20).std()\n",
    "    \n",
    "    # Realized volatility\n",
    "    stats['realized_vol'] = stats['underlying_returns'].rolling(window=20).std() * np.sqrt(252)\n",
    "    \n",
    "    # Volatility skew\n",
    "    stats['vol_skew'] = stats['underlying_returns'].rolling(window=20).skew()\n",
    "    \n",
    "    # Volatility kurtosis\n",
    "    stats['vol_kurt'] = stats['underlying_returns'].rolling(window=20).kurt()\n",
    "    \n",
    "    # Volatility term structure\n",
    "    for window in [5, 10, 20, 60]:\n",
    "        stats[f'vol_term_{window}'] = stats['underlying_returns'].rolling(window=window).std() * np.sqrt(252)\n",
    "    \n",
    "    # Fill missing numerical values first\n",
    "    numerical_cols = stats.select_dtypes(include=[np.number]).columns\n",
    "    stats[numerical_cols] = stats[numerical_cols].fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "    \n",
    "    # Now create volatility regime after filling numerical values\n",
    "    try:\n",
    "        # Handle edge case where all values might be the same\n",
    "        unique_vals = stats['realized_vol'].nunique()\n",
    "        if unique_vals >= 5:\n",
    "            stats['vol_regime'] = pd.qcut(stats['realized_vol'], q=5, labels=['very_low', 'low', 'medium', 'high', 'very_high'])\n",
    "        else:\n",
    "            # If not enough unique values, use a simpler classification\n",
    "            stats['vol_regime'] = pd.cut(stats['realized_vol'], \n",
    "                                        bins=[-np.inf, stats['realized_vol'].quantile(0.33), \n",
    "                                              stats['realized_vol'].quantile(0.67), np.inf],\n",
    "                                        labels=['low', 'medium', 'high'])\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create volatility regime categories: {e}\")\n",
    "        stats['vol_regime'] = 'medium'  # Default to medium if categorization fails\n",
    "    \n",
    "    print(f\"Created {len(stats.columns)} statistical features\")\n",
    "    return stats\n",
    "\n",
    "# Test statistics calculation on a small sample\n",
    "sample_stats = calculate_advanced_statistics(train.head(100))\n",
    "print(f\"\\nSample statistics shape: {sample_stats.shape}\")\n",
    "print(\"\\nStatistical features:\")\n",
    "print(sample_stats.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_iv(data):\n",
    "    print(\"Starting SVI IV prediction...\")\n",
    "    data = data.copy()\n",
    "    \n",
    "    # Phase 1: Put-call parity\n",
    "    print(\"\\nPhase 1: Applying put-call parity...\")\n",
    "    for strike, cols in strike_dict.items():\n",
    "        call_col = cols['call']\n",
    "        put_col = cols['put']\n",
    "        \n",
    "        if call_col is not None and put_col is not None and call_col in data.columns and put_col in data.columns:\n",
    "            call_mask = data[call_col].isna() & data[put_col].notna()\n",
    "            data.loc[call_mask, call_col] = data.loc[call_mask, put_col]\n",
    "            \n",
    "            put_mask = data[put_col].isna() & data[call_col].notna()\n",
    "            data.loc[put_mask, put_col] = data.loc[put_mask, call_col]\n",
    "    \n",
    "    # Phase 2: Calculate advanced statistics\n",
    "    print(\"\\nPhase 2: Calculating advanced statistics...\")\n",
    "    stats = calculate_advanced_statistics(data)\n",
    "    \n",
    "    # Phase 3: SVI fitting and prediction\n",
    "    print(\"\\nPhase 3: Fitting SVI models...\")\n",
    "    for idx, row in data.iterrows():\n",
    "        for strike, cols in strike_dict.items():\n",
    "            strike_price = float(strike)\n",
    "            call_col = cols['call']\n",
    "            put_col = cols['put']\n",
    "            \n",
    "            if call_col is not None and put_col is not None and call_col in data.columns and put_col in data.columns:\n",
    "                # Get available IVs for this timestamp\n",
    "                available_strikes = []\n",
    "                available_ivs = []\n",
    "                \n",
    "                for s, c in strike_dict.items():\n",
    "                    if c['call'] is not None and not np.isnan(data.at[idx, c['call']]):\n",
    "                        available_strikes.append(float(s))\n",
    "                        available_ivs.append(data.at[idx, c['call']])\n",
    "                    if c['put'] is not None and not np.isnan(data.at[idx, c['put']]):\n",
    "                        available_strikes.append(float(s))\n",
    "                        available_ivs.append(data.at[idx, c['put']])\n",
    "                \n",
    "                if len(available_strikes) >= 3:\n",
    "                    # Fit SVI model\n",
    "                    k = np.log(np.array(available_strikes) / row['underlying'])\n",
    "                    w = np.array(available_ivs)**2\n",
    "                    \n",
    "                    try:\n",
    "                        params = fit_svi(k, w)\n",
    "                        \n",
    "                        # Predict missing IVs\n",
    "                        if np.isnan(data.at[idx, call_col]):\n",
    "                            k_pred = np.log(strike_price / row['underlying'])\n",
    "                            w_pred = svi(k_pred, *params)\n",
    "                            data.at[idx, call_col] = np.sqrt(w_pred)\n",
    "                        \n",
    "                        if np.isnan(data.at[idx, put_col]):\n",
    "                            k_pred = np.log(strike_price / row['underlying'])\n",
    "                            w_pred = svi(k_pred, *params)\n",
    "                            data.at[idx, put_col] = np.sqrt(w_pred)\n",
    "                    except:\n",
    "                        # If SVI fails, use nearest available IV\n",
    "                        if np.isnan(data.at[idx, call_col]):\n",
    "                            if available_strikes:  # Check if we have any available strikes\n",
    "                                nearest_idx = np.argmin(np.abs(np.array(available_strikes) - strike_price))\n",
    "                                data.at[idx, call_col] = available_ivs[nearest_idx]\n",
    "                        \n",
    "                        if np.isnan(data.at[idx, put_col]):\n",
    "                            if not np.isnan(data.at[idx, call_col]):\n",
    "                                data.at[idx, put_col] = data.at[idx, call_col]\n",
    "    \n",
    "    # Phase 4: Smoothing and consistency\n",
    "    print(\"\\nPhase 4: Applying smoothing and consistency checks...\")\n",
    "    for idx, row in data.iterrows():\n",
    "        for strike, cols in strike_dict.items():\n",
    "            call_col = cols['call']\n",
    "            put_col = cols['put']\n",
    "            \n",
    "            if call_col is not None and put_col is not None and call_col in data.columns and put_col in data.columns:\n",
    "                if not np.isnan(data.at[idx, call_col]) and not np.isnan(data.at[idx, put_col]):\n",
    "                    avg_iv = (data.at[idx, call_col] + data.at[idx, put_col]) / 2\n",
    "                    data.at[idx, call_col] = 0.9 * data.at[idx, call_col] + 0.1 * avg_iv\n",
    "                    data.at[idx, put_col] = 0.9 * data.at[idx, put_col] + 0.1 * avg_iv\n",
    "    \n",
    "    # Ensure all values are within reasonable bounds\n",
    "    for col in iv_columns:\n",
    "        if col in data.columns:\n",
    "            data[col] = np.clip(data[col], 0.01, 1.0)\n",
    "    \n",
    "    print(\"\\nPrediction completed successfully\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation split...\n",
      "Training set size: 142672\n",
      "Validation set size: 35668\n",
      "\n",
      "Running validation...\n",
      "Starting SVI IV prediction...\n",
      "\n",
      "Phase 1: Applying put-call parity...\n",
      "\n",
      "Phase 2: Calculating advanced statistics...\n",
      "Calculating advanced statistics...\n",
      "Created 117 statistical features\n",
      "\n",
      "Phase 3: Fitting SVI models...\n",
      "Created 117 statistical features\n",
      "\n",
      "Phase 3: Fitting SVI models...\n"
     ]
    }
   ],
   "source": [
    "# Create validation split\n",
    "print(\"Creating validation split...\")\n",
    "train_df, val_df = train_test_split(train, test_size=0.2, random_state=42)\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "\n",
    "# Add any missing IV columns to validation set\n",
    "for col in iv_columns:\n",
    "    if col not in val_df.columns:\n",
    "        val_df[col] = np.nan\n",
    "\n",
    "# Apply to validation set\n",
    "print(\"\\nRunning validation...\")\n",
    "val_pred = predict_iv(val_df)\n",
    "\n",
    "# Calculate MSE only on originally masked validation points\n",
    "mse_vals = []\n",
    "for col in iv_columns:\n",
    "    if col in val_df.columns and col in val_pred.columns:\n",
    "        mask = val_df[col].isna() & val_pred[col].notna()\n",
    "        if mask.any():\n",
    "            se = (val_df.loc[mask, col] - val_pred.loc[mask, col]) ** 2\n",
    "            mse_vals.append(se.mean())\n",
    "\n",
    "validation_mse = np.mean(mse_vals) if mse_vals else 0\n",
    "print(f\"\\nValidation MSE (masked points only): {validation_mse:.12f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to test set\n",
    "print(\"Generating final predictions...\")\n",
    "test_pred = predict_iv(test)\n",
    "\n",
    "# Prepare submission\n",
    "submission = test_pred[['timestamp'] + iv_columns].copy()\n",
    "submission.columns = sample_sub.columns\n",
    "\n",
    "# Verify no missing values\n",
    "assert submission.isna().sum().sum() == 0, \"Missing values detected\"\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\nFinal Submission Preview:\")\n",
    "print(submission.head())\n",
    "print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "print(f\"Validation MSE: {validation_mse:.12f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
