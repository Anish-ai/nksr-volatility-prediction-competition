{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volatility Curve Predictor\n",
    "\n",
    "This notebook implements a volatility curve prediction model using advanced interpolation techniques and statistical methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.interpolate import interp1d, splrep, splev\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Train data shape: (178340, 97)\n",
      "Test data shape: (12065, 96)\n",
      "Sample submission shape: (12065, 53)\n",
      "\n",
      "Number of IV columns: 52\n",
      "\n",
      "Number of unique strikes: 36\n",
      "\n",
      "Strike dictionary:\n",
      "{'24000': {'call': 'call_iv_24000', 'put': 'put_iv_24000'}, '24100': {'call': 'call_iv_24100', 'put': 'put_iv_24100'}, '24200': {'call': 'call_iv_24200', 'put': 'put_iv_24200'}, '24300': {'call': 'call_iv_24300', 'put': 'put_iv_24300'}, '24400': {'call': 'call_iv_24400', 'put': 'put_iv_24400'}, '24500': {'call': 'call_iv_24500', 'put': 'put_iv_24500'}, '24600': {'call': 'call_iv_24600', 'put': 'put_iv_24600'}, '24700': {'call': 'call_iv_24700', 'put': 'put_iv_24700'}, '24800': {'call': 'call_iv_24800', 'put': 'put_iv_24800'}, '24900': {'call': 'call_iv_24900', 'put': 'put_iv_24900'}, '25000': {'call': 'call_iv_25000', 'put': 'put_iv_25000'}, '25100': {'call': 'call_iv_25100', 'put': 'put_iv_25100'}, '25200': {'call': 'call_iv_25200', 'put': 'put_iv_25200'}, '25300': {'call': 'call_iv_25300', 'put': 'put_iv_25300'}, '25400': {'call': 'call_iv_25400', 'put': 'put_iv_25400'}, '25500': {'call': 'call_iv_25500', 'put': 'put_iv_25500'}, '25600': {'call': 'call_iv_25600', 'put': None}, '25700': {'call': 'call_iv_25700', 'put': None}, '25800': {'call': 'call_iv_25800', 'put': None}, '25900': {'call': 'call_iv_25900', 'put': None}, '26000': {'call': 'call_iv_26000', 'put': None}, '26100': {'call': 'call_iv_26100', 'put': None}, '26200': {'call': 'call_iv_26200', 'put': None}, '26300': {'call': 'call_iv_26300', 'put': None}, '26400': {'call': 'call_iv_26400', 'put': None}, '26500': {'call': 'call_iv_26500', 'put': None}, '23000': {'call': None, 'put': 'put_iv_23000'}, '23100': {'call': None, 'put': 'put_iv_23100'}, '23200': {'call': None, 'put': 'put_iv_23200'}, '23300': {'call': None, 'put': 'put_iv_23300'}, '23400': {'call': None, 'put': 'put_iv_23400'}, '23500': {'call': None, 'put': 'put_iv_23500'}, '23600': {'call': None, 'put': 'put_iv_23600'}, '23700': {'call': None, 'put': 'put_iv_23700'}, '23800': {'call': None, 'put': 'put_iv_23800'}, '23900': {'call': None, 'put': 'put_iv_23900'}}\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "train = pd.read_parquet('train_data.parquet')\n",
    "test = pd.read_parquet('test_data.parquet')\n",
    "sample_sub = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "print(f\"\\nTrain data shape: {train.shape}\")\n",
    "print(f\"Test data shape: {test.shape}\")\n",
    "print(f\"Sample submission shape: {sample_sub.shape}\")\n",
    "\n",
    "# Get all IV columns from TEST data\n",
    "iv_columns = [col for col in test.columns if col.startswith(('call_iv_', 'put_iv_'))]\n",
    "print(f\"\\nNumber of IV columns: {len(iv_columns)}\")\n",
    "\n",
    "# Create strike dictionary from TEST columns\n",
    "strike_dict = {}\n",
    "for col in iv_columns:\n",
    "    strike = col.split('_')[-1]\n",
    "    if strike not in strike_dict:\n",
    "        strike_dict[strike] = {'call': None, 'put': None}\n",
    "    \n",
    "    if col.startswith('call_iv_'):\n",
    "        strike_dict[strike]['call'] = col\n",
    "    else:\n",
    "        strike_dict[strike]['put'] = col\n",
    "\n",
    "print(f\"\\nNumber of unique strikes: {len(strike_dict)}\")\n",
    "print(\"\\nStrike dictionary:\")\n",
    "print(strike_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Statistical Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating advanced statistics...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot setitem on a Categorical with a new category (0), set the categories first",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stats\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Test statistics calculation on a small sample\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m sample_stats \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_advanced_statistics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSample statistics shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_stats\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStatistical features:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 28\u001b[0m, in \u001b[0;36mcalculate_advanced_statistics\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     25\u001b[0m stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvol_regime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mqcut(stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrealized_vol\u001b[39m\u001b[38;5;124m'\u001b[39m], q\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvery_low\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlow\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedium\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigh\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvery_high\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Fill missing values\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mffill\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbfill\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(stats\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m statistical features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stats\n",
      "File \u001b[1;32md:\\Volatility\\venv\\lib\\site-packages\\pandas\\core\\generic.py:7434\u001b[0m, in \u001b[0;36mNDFrame.fillna\u001b[1;34m(self, value, method, axis, inplace, limit, downcast)\u001b[0m\n\u001b[0;32m   7432\u001b[0m         new_data \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m_mgr\n\u001b[0;32m   7433\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 7434\u001b[0m         new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   7435\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdowncast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdowncast\u001b[49m\n\u001b[0;32m   7436\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   7437\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, ABCDataFrame) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   7438\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhere(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotna(), value)\u001b[38;5;241m.\u001b[39m_mgr\n",
      "File \u001b[1;32md:\\Volatility\\venv\\lib\\site-packages\\pandas\\core\\internals\\base.py:186\u001b[0m, in \u001b[0;36mDataManager.fillna\u001b[1;34m(self, value, limit, inplace, downcast)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Do this validation even if we go through one of the no-op paths\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     limit \u001b[38;5;241m=\u001b[39m libalgos\u001b[38;5;241m.\u001b[39mvalidate_limit(\u001b[38;5;28;01mNone\u001b[39;00m, limit\u001b[38;5;241m=\u001b[39mlimit)\n\u001b[1;32m--> 186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_with_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfillna\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdowncast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdowncast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43malready_warned\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_AlreadyWarned\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Volatility\\venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32md:\\Volatility\\venv\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:2334\u001b[0m, in \u001b[0;36mExtensionBlock.fillna\u001b[1;34m(self, value, limit, inplace, downcast, using_cow, already_warned)\u001b[0m\n\u001b[0;32m   2331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   2332\u001b[0m     \u001b[38;5;66;03m# 3rd party EA that has not implemented copy keyword yet\u001b[39;00m\n\u001b[0;32m   2333\u001b[0m     refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2334\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2335\u001b[0m     \u001b[38;5;66;03m# issue the warning *after* retrying, in case the TypeError\u001b[39;00m\n\u001b[0;32m   2336\u001b[0m     \u001b[38;5;66;03m#  was caused by an invalid fill_value\u001b[39;00m\n\u001b[0;32m   2337\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2338\u001b[0m         \u001b[38;5;66;03m# GH#53278\u001b[39;00m\n\u001b[0;32m   2339\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtensionArray.fillna added a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcopy\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keyword in pandas \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2345\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   2346\u001b[0m     )\n",
      "File \u001b[1;32md:\\Volatility\\venv\\lib\\site-packages\\pandas\\core\\arrays\\_mixins.py:376\u001b[0m, in \u001b[0;36mNDArrayBackedExtensionArray.fillna\u001b[1;34m(self, value, method, limit, copy)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;66;03m# We validate the fill_value even if there is nothing to fill\u001b[39;00m\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 376\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_setitem_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m copy:\n\u001b[0;32m    379\u001b[0m         new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m[:]\n",
      "File \u001b[1;32md:\\Volatility\\venv\\lib\\site-packages\\pandas\\core\\arrays\\categorical.py:1589\u001b[0m, in \u001b[0;36mCategorical._validate_setitem_value\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   1587\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_listlike(value)\n\u001b[0;32m   1588\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_scalar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Volatility\\venv\\lib\\site-packages\\pandas\\core\\arrays\\categorical.py:1614\u001b[0m, in \u001b[0;36mCategorical._validate_scalar\u001b[1;34m(self, fill_value)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     fill_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unbox_scalar(fill_value)\n\u001b[0;32m   1613\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1614\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   1615\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot setitem on a Categorical with a new \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1616\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfill_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m), set the categories first\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1617\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1618\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fill_value\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot setitem on a Categorical with a new category (0), set the categories first"
     ]
    }
   ],
   "source": [
    "def calculate_advanced_statistics(df):\n",
    "    \"\"\"Calculate advanced statistical metrics\"\"\"\n",
    "    print(\"Calculating advanced statistics...\")\n",
    "    stats = df.copy()\n",
    "\n",
    "    stats['underlying_returns'] = np.log(stats['underlying'] / stats['underlying'].shift(1))\n",
    "    \n",
    "    # Volatility of volatility\n",
    "    stats['vol_of_vol'] = stats['underlying_returns'].rolling(window=20).std().rolling(window=20).std()\n",
    "    \n",
    "    # Realized volatility\n",
    "    stats['realized_vol'] = stats['underlying_returns'].rolling(window=20).std() * np.sqrt(252)\n",
    "    \n",
    "    # Volatility skew\n",
    "    stats['vol_skew'] = stats['underlying_returns'].rolling(window=20).skew()\n",
    "    \n",
    "    # Volatility kurtosis\n",
    "    stats['vol_kurt'] = stats['underlying_returns'].rolling(window=20).kurt()\n",
    "    \n",
    "    # Volatility term structure\n",
    "    for window in [5, 10, 20, 60]:\n",
    "        stats[f'vol_term_{window}'] = stats['underlying_returns'].rolling(window=window).std() * np.sqrt(252)\n",
    "    \n",
    "    # Fill numerical values first\n",
    "    numerical_cols = stats.select_dtypes(include=[np.number]).columns\n",
    "    stats[numerical_cols] = stats[numerical_cols].fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "    \n",
    "    # Volatility regime indicators - create after filling numerical values\n",
    "    try:\n",
    "        stats['vol_regime'] = pd.qcut(stats['realized_vol'], q=5, labels=['very_low', 'low', 'medium', 'high', 'very_high'])\n",
    "    except ValueError:\n",
    "        # Handle case when there are not enough unique values\n",
    "        stats['vol_regime'] = pd.Series(['medium'] * len(stats), index=stats.index)\n",
    "    \n",
    "    # Fill any remaining categorical columns with their mode\n",
    "    categorical_cols = stats.select_dtypes(include=['category']).columns\n",
    "    for col in categorical_cols:\n",
    "        mode_val = stats[col].mode()[0] if not stats[col].mode().empty else None\n",
    "        stats[col] = stats[col].fillna(mode_val)\n",
    "    \n",
    "    print(f\"Created {len(stats.columns)} statistical features\")\n",
    "    return stats\n",
    "\n",
    "# Test statistics calculation on a small sample\n",
    "sample_stats = calculate_advanced_statistics(train.head(100))\n",
    "print(f\"\\nSample statistics shape: {sample_stats.shape}\")\n",
    "print(\"\\nStatistical features:\")\n",
    "print(sample_stats.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Curve Interpolation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_curve(x, y, x_new, method='spline'):\n",
    "    \"\"\"Interpolate volatility curve using different methods\"\"\"\n",
    "    if method == 'spline':\n",
    "        # Cubic spline interpolation\n",
    "        tck = splrep(x, y, k=3)\n",
    "        return splev(x_new, tck)\n",
    "    elif method == 'linear':\n",
    "        # Linear interpolation\n",
    "        f = interp1d(x, y, kind='linear', bounds_error=False, fill_value=(y[0], y[-1]))\n",
    "        return f(x_new)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown interpolation method: {method}\")\n",
    "\n",
    "print(\"Curve interpolation functions defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_iv(data):\n",
    "    print(\"Starting volatility curve prediction...\")\n",
    "    data = data.copy()\n",
    "    \n",
    "    # Phase 1: Put-call parity\n",
    "    print(\"\\nPhase 1: Applying put-call parity...\")\n",
    "    for strike, cols in strike_dict.items():\n",
    "        call_col = cols['call']\n",
    "        put_col = cols['put']\n",
    "        \n",
    "        if call_col in data.columns and put_col in data.columns:\n",
    "            call_mask = data[call_col].isna() & data[put_col].notna()\n",
    "            data.loc[call_mask, call_col] = data.loc[call_mask, put_col]\n",
    "            \n",
    "            put_mask = data[put_col].isna() & data[call_col].notna()\n",
    "            data.loc[put_mask, put_col] = data.loc[put_mask, call_col]\n",
    "    \n",
    "    # Phase 2: Calculate advanced statistics\n",
    "    print(\"\\nPhase 2: Calculating advanced statistics...\")\n",
    "    stats = calculate_advanced_statistics(data)\n",
    "    \n",
    "    # Phase 3: Curve interpolation\n",
    "    print(\"\\nPhase 3: Performing curve interpolation...\")\n",
    "    for idx, row in data.iterrows():\n",
    "        for strike, cols in strike_dict.items():\n",
    "            strike_price = float(strike)\n",
    "            call_col = cols['call']\n",
    "            put_col = cols['put']\n",
    "            \n",
    "            if call_col in data.columns and put_col in data.columns:\n",
    "                # Get available IVs for this timestamp\n",
    "                available_strikes = []\n",
    "                available_ivs = []\n",
    "                \n",
    "                for s, c in strike_dict.items():\n",
    "                    if not np.isnan(data.at[idx, c['call']]):\n",
    "                        available_strikes.append(float(s))\n",
    "                        available_ivs.append(data.at[idx, c['call']])\n",
    "                    if not np.isnan(data.at[idx, c['put']]):\n",
    "                        available_strikes.append(float(s))\n",
    "                        available_ivs.append(data.at[idx, c['put']])\n",
    "                \n",
    "                if len(available_strikes) >= 3:\n",
    "                    try:\n",
    "                        # Try spline interpolation first\n",
    "                        iv_pred = interpolate_curve(available_strikes, available_ivs, strike_price, method='spline')\n",
    "                        \n",
    "                        if np.isnan(data.at[idx, call_col]):\n",
    "                            data.at[idx, call_col] = iv_pred\n",
    "                        \n",
    "                        if np.isnan(data.at[idx, put_col]):\n",
    "                            data.at[idx, put_col] = iv_pred\n",
    "                    except:\n",
    "                        try:\n",
    "                            # Fallback to linear interpolation\n",
    "                            iv_pred = interpolate_curve(available_strikes, available_ivs, strike_price, method='linear')\n",
    "                            \n",
    "                            if np.isnan(data.at[idx, call_col]):\n",
    "                                data.at[idx, call_col] = iv_pred\n",
    "                            \n",
    "                            if np.isnan(data.at[idx, put_col]):\n",
    "                                data.at[idx, put_col] = iv_pred\n",
    "                        except:\n",
    "                            # If both methods fail, use nearest available IV\n",
    "                            if np.isnan(data.at[idx, call_col]):\n",
    "                                nearest_idx = np.argmin(np.abs(np.array(available_strikes) - strike_price))\n",
    "                                data.at[idx, call_col] = available_ivs[nearest_idx]\n",
    "                            \n",
    "                            if np.isnan(data.at[idx, put_col]):\n",
    "                                data.at[idx, put_col] = data.at[idx, call_col]\n",
    "    \n",
    "    # Phase 4: Smoothing and consistency\n",
    "    print(\"\\nPhase 4: Applying smoothing and consistency checks...\")\n",
    "    for idx, row in data.iterrows():\n",
    "        for strike, cols in strike_dict.items():\n",
    "            call_col = cols['call']\n",
    "            put_col = cols['put']\n",
    "            \n",
    "            if call_col in data.columns and put_col in data.columns:\n",
    "                avg_iv = (data.at[idx, call_col] + data.at[idx, put_col]) / 2\n",
    "                data.at[idx, call_col] = 0.9 * data.at[idx, call_col] + 0.1 * avg_iv\n",
    "                data.at[idx, put_col] = 0.9 * data.at[idx, put_col] + 0.1 * avg_iv\n",
    "    \n",
    "    # Ensure all values are within reasonable bounds\n",
    "    for col in iv_columns:\n",
    "        if col in data.columns:\n",
    "            data[col] = np.clip(data[col], 0.01, 1.0)\n",
    "    \n",
    "    print(\"\\nPrediction completed successfully\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation split\n",
    "print(\"Creating validation split...\")\n",
    "train_df, val_df = train_test_split(train, test_size=0.2, random_state=42)\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "\n",
    "# Apply to validation set\n",
    "print(\"\\nRunning validation...\")\n",
    "val_pred = predict_iv(val_df)\n",
    "\n",
    "# Calculate MSE only on originally masked validation points\n",
    "mse_vals = []\n",
    "for col in iv_columns:\n",
    "    if col in val_df.columns and col in val_pred.columns:\n",
    "        mask = val_df[col].isna() & val_pred[col].notna()\n",
    "        if mask.any():\n",
    "            se = (val_df.loc[mask, col] - val_pred.loc[mask, col]) ** 2\n",
    "            mse_vals.append(se.mean())\n",
    "\n",
    "validation_mse = np.mean(mse_vals) if mse_vals else 0\n",
    "print(f\"\\nValidation MSE (masked points only): {validation_mse:.12f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to test set\n",
    "print(\"Generating final predictions...\")\n",
    "test_pred = predict_iv(test)\n",
    "\n",
    "# Prepare submission\n",
    "submission = test_pred[['timestamp'] + iv_columns].copy()\n",
    "submission.columns = sample_sub.columns\n",
    "\n",
    "# Verify no missing values\n",
    "assert submission.isna().sum().sum() == 0, \"Missing values detected\"\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\nFinal Submission Preview:\")\n",
    "print(submission.head())\n",
    "print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "print(f\"Validation MSE: {validation_mse:.12f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
