{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Volatility Predictor\n",
    "\n",
    "This notebook implements a simple volatility prediction model using basic statistical methods and interpolation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Train data shape: (178340, 97)\n",
      "Test data shape: (12065, 96)\n",
      "Sample submission shape: (12065, 53)\n",
      "\n",
      "Number of IV columns: 52\n",
      "\n",
      "Number of unique strikes: 36\n",
      "\n",
      "Strike dictionary:\n",
      "{'24000': {'call': 'call_iv_24000', 'put': 'put_iv_24000'}, '24100': {'call': 'call_iv_24100', 'put': 'put_iv_24100'}, '24200': {'call': 'call_iv_24200', 'put': 'put_iv_24200'}, '24300': {'call': 'call_iv_24300', 'put': 'put_iv_24300'}, '24400': {'call': 'call_iv_24400', 'put': 'put_iv_24400'}, '24500': {'call': 'call_iv_24500', 'put': 'put_iv_24500'}, '24600': {'call': 'call_iv_24600', 'put': 'put_iv_24600'}, '24700': {'call': 'call_iv_24700', 'put': 'put_iv_24700'}, '24800': {'call': 'call_iv_24800', 'put': 'put_iv_24800'}, '24900': {'call': 'call_iv_24900', 'put': 'put_iv_24900'}, '25000': {'call': 'call_iv_25000', 'put': 'put_iv_25000'}, '25100': {'call': 'call_iv_25100', 'put': 'put_iv_25100'}, '25200': {'call': 'call_iv_25200', 'put': 'put_iv_25200'}, '25300': {'call': 'call_iv_25300', 'put': 'put_iv_25300'}, '25400': {'call': 'call_iv_25400', 'put': 'put_iv_25400'}, '25500': {'call': 'call_iv_25500', 'put': 'put_iv_25500'}, '25600': {'call': 'call_iv_25600', 'put': None}, '25700': {'call': 'call_iv_25700', 'put': None}, '25800': {'call': 'call_iv_25800', 'put': None}, '25900': {'call': 'call_iv_25900', 'put': None}, '26000': {'call': 'call_iv_26000', 'put': None}, '26100': {'call': 'call_iv_26100', 'put': None}, '26200': {'call': 'call_iv_26200', 'put': None}, '26300': {'call': 'call_iv_26300', 'put': None}, '26400': {'call': 'call_iv_26400', 'put': None}, '26500': {'call': 'call_iv_26500', 'put': None}, '23000': {'call': None, 'put': 'put_iv_23000'}, '23100': {'call': None, 'put': 'put_iv_23100'}, '23200': {'call': None, 'put': 'put_iv_23200'}, '23300': {'call': None, 'put': 'put_iv_23300'}, '23400': {'call': None, 'put': 'put_iv_23400'}, '23500': {'call': None, 'put': 'put_iv_23500'}, '23600': {'call': None, 'put': 'put_iv_23600'}, '23700': {'call': None, 'put': 'put_iv_23700'}, '23800': {'call': None, 'put': 'put_iv_23800'}, '23900': {'call': None, 'put': 'put_iv_23900'}}\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "train = pd.read_parquet('train_data.parquet')\n",
    "test = pd.read_parquet('test_data.parquet')\n",
    "sample_sub = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "print(f\"\\nTrain data shape: {train.shape}\")\n",
    "print(f\"Test data shape: {test.shape}\")\n",
    "print(f\"Sample submission shape: {sample_sub.shape}\")\n",
    "\n",
    "# Get all IV columns from TEST data\n",
    "iv_columns = [col for col in test.columns if col.startswith(('call_iv_', 'put_iv_'))]\n",
    "print(f\"\\nNumber of IV columns: {len(iv_columns)}\")\n",
    "\n",
    "# Create strike dictionary from TEST columns\n",
    "strike_dict = {}\n",
    "for col in iv_columns:\n",
    "    strike = col.split('_')[-1]\n",
    "    if strike not in strike_dict:\n",
    "        strike_dict[strike] = {'call': None, 'put': None}\n",
    "    \n",
    "    if col.startswith('call_iv_'):\n",
    "        strike_dict[strike]['call'] = col\n",
    "    else:\n",
    "        strike_dict[strike]['put'] = col\n",
    "\n",
    "print(f\"\\nNumber of unique strikes: {len(strike_dict)}\")\n",
    "print(\"\\nStrike dictionary:\")\n",
    "print(strike_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Statistical Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating basic statistics...\n",
      "Created 101 statistical features\n",
      "\n",
      "Sample statistics shape: (100, 101)\n",
      "\n",
      "Statistical features:\n",
      "['timestamp', 'underlying', 'expiry', 'call_iv_23500', 'call_iv_23600', 'call_iv_23700', 'call_iv_23800', 'call_iv_23900', 'call_iv_24000', 'call_iv_24100', 'call_iv_24200', 'call_iv_24300', 'call_iv_24400', 'call_iv_24500', 'call_iv_24600', 'call_iv_24700', 'call_iv_24800', 'call_iv_24900', 'call_iv_25000', 'call_iv_25100', 'call_iv_25200', 'call_iv_25300', 'call_iv_25400', 'call_iv_25500', 'call_iv_25600', 'call_iv_25700', 'call_iv_25800', 'call_iv_25900', 'call_iv_26000', 'put_iv_22500', 'put_iv_22600', 'put_iv_22700', 'put_iv_22800', 'put_iv_22900', 'put_iv_23000', 'put_iv_23100', 'put_iv_23200', 'put_iv_23300', 'put_iv_23400', 'put_iv_23500', 'put_iv_23600', 'put_iv_23700', 'put_iv_23800', 'put_iv_23900', 'put_iv_24000', 'put_iv_24100', 'put_iv_24200', 'put_iv_24300', 'put_iv_24400', 'put_iv_24500', 'put_iv_24600', 'put_iv_24700', 'put_iv_24800', 'put_iv_24900', 'put_iv_25000', 'X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'X24', 'X25', 'X26', 'X27', 'X28', 'X29', 'X30', 'X31', 'X32', 'X33', 'X34', 'X35', 'X36', 'X37', 'X38', 'X39', 'X40', 'X41', 'underlying_returns', 'realized_vol', 'vol_skew', 'vol_kurt']\n"
     ]
    }
   ],
   "source": [
    "def calculate_basic_statistics(df):\n",
    "    \"\"\"Calculate basic statistical metrics\"\"\"\n",
    "    print(\"Calculating basic statistics...\")\n",
    "    stats = df.copy()\n",
    "    \n",
    "    # Calculate returns first\n",
    "    stats['underlying_returns'] = np.log(stats['underlying'] / stats['underlying'].shift(1))\n",
    "    \n",
    "    # Realized volatility\n",
    "    stats['realized_vol'] = stats['underlying_returns'].rolling(window=20).std() * np.sqrt(252)\n",
    "    \n",
    "    # Volatility skew\n",
    "    stats['vol_skew'] = stats['underlying_returns'].rolling(window=20).skew()\n",
    "    \n",
    "    # Volatility kurtosis\n",
    "    stats['vol_kurt'] = stats['underlying_returns'].rolling(window=20).kurt()\n",
    "    \n",
    "    # Fill missing values\n",
    "    stats = stats.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "    \n",
    "    print(f\"Created {len(stats.columns)} statistical features\")\n",
    "    return stats\n",
    "\n",
    "# Test statistics calculation on a small sample\n",
    "sample_stats = calculate_basic_statistics(train.head(100))\n",
    "print(f\"\\nSample statistics shape: {sample_stats.shape}\")\n",
    "print(\"\\nStatistical features:\")\n",
    "print(sample_stats.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simple Interpolation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple interpolation function defined successfully\n"
     ]
    }
   ],
   "source": [
    "def simple_interpolation(x, y, x_new):\n",
    "    \"\"\"Perform simple linear interpolation\"\"\"\n",
    "    return np.interp(x_new, x, y)\n",
    "\n",
    "print(\"Simple interpolation function defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_iv(data):\n",
    "    print(\"Starting simple volatility prediction...\")\n",
    "    data = data.copy()\n",
    "    \n",
    "    # Add missing IV columns from test set\n",
    "    for col in iv_columns:\n",
    "        if col not in data.columns:\n",
    "            data[col] = np.nan\n",
    "    \n",
    "    # Phase 1: Put-call parity\n",
    "    print(\"\\nPhase 1: Applying put-call parity...\")\n",
    "    for strike, cols in strike_dict.items():\n",
    "        call_col = cols['call']\n",
    "        put_col = cols['put']\n",
    "        \n",
    "        # Ensure both columns exist\n",
    "        if call_col not in data.columns:\n",
    "            data[call_col] = np.nan\n",
    "        if put_col not in data.columns:\n",
    "            data[put_col] = np.nan\n",
    "        \n",
    "        call_mask = data[call_col].isna() & data[put_col].notna()\n",
    "        data.loc[call_mask, call_col] = data.loc[call_mask, put_col]\n",
    "        \n",
    "        put_mask = data[put_col].isna() & data[call_col].notna()\n",
    "        data.loc[put_mask, put_col] = data.loc[put_mask, call_col]\n",
    "    \n",
    "    # Phase 2: Calculate basic statistics\n",
    "    print(\"\\nPhase 2: Calculating basic statistics...\")\n",
    "    stats = calculate_basic_statistics(data)\n",
    "    \n",
    "    # Phase 3: Simple interpolation\n",
    "    print(\"\\nPhase 3: Performing simple interpolation...\")\n",
    "    for idx, row in data.iterrows():\n",
    "        # Get available IVs for this timestamp\n",
    "        available_strikes = []\n",
    "        available_ivs = []\n",
    "        \n",
    "        # Collect all available IVs\n",
    "        for s, c in strike_dict.items():\n",
    "            for col_type in ['call', 'put']:\n",
    "                col = c[col_type]\n",
    "                if not np.isnan(data.at[idx, col]):\n",
    "                    available_strikes.append(float(s))\n",
    "                    available_ivs.append(data.at[idx, col])\n",
    "        \n",
    "        # Sort by strike price for proper interpolation\n",
    "        if len(available_strikes) >= 2:\n",
    "            sort_idx = np.argsort(available_strikes)\n",
    "            available_strikes = np.array(available_strikes)[sort_idx]\n",
    "            available_ivs = np.array(available_ivs)[sort_idx]\n",
    "            \n",
    "            # For each missing IV, interpolate using available values\n",
    "            for strike, cols in strike_dict.items():\n",
    "                strike_price = float(strike)\n",
    "                call_col = cols['call']\n",
    "                put_col = cols['put']\n",
    "                \n",
    "                try:\n",
    "                    if strike_price < min(available_strikes):\n",
    "                        # Use nearest value for extrapolation\n",
    "                        iv_pred = available_ivs[0]\n",
    "                    elif strike_price > max(available_strikes):\n",
    "                        # Use nearest value for extrapolation\n",
    "                        iv_pred = available_ivs[-1]\n",
    "                    else:\n",
    "                        # Interpolate\n",
    "                        iv_pred = simple_interpolation(available_strikes, available_ivs, strike_price)\n",
    "                    \n",
    "                    if np.isnan(data.at[idx, call_col]):\n",
    "                        data.at[idx, call_col] = iv_pred\n",
    "                    \n",
    "                    if np.isnan(data.at[idx, put_col]):\n",
    "                        data.at[idx, put_col] = iv_pred\n",
    "                except:\n",
    "                    # If interpolation fails, use nearest available IV\n",
    "                    if np.isnan(data.at[idx, call_col]) or np.isnan(data.at[idx, put_col]):\n",
    "                        nearest_idx = np.argmin(np.abs(np.array(available_strikes) - strike_price))\n",
    "                        iv_pred = available_ivs[nearest_idx]\n",
    "                        \n",
    "                        if np.isnan(data.at[idx, call_col]):\n",
    "                            data.at[idx, call_col] = iv_pred\n",
    "                        \n",
    "                        if np.isnan(data.at[idx, put_col]):\n",
    "                            data.at[idx, put_col] = iv_pred\n",
    "        else:\n",
    "            # Not enough points for interpolation, use realized volatility as fallback\n",
    "            realized_vol = stats.at[idx, 'realized_vol']\n",
    "            default_iv = min(max(realized_vol, 0.01), 1.0) if not np.isnan(realized_vol) else 0.2  # Use 0.2 as last resort\n",
    "            \n",
    "            for strike, cols in strike_dict.items():\n",
    "                call_col = cols['call']\n",
    "                put_col = cols['put']\n",
    "                \n",
    "                if np.isnan(data.at[idx, call_col]):\n",
    "                    data.at[idx, call_col] = default_iv\n",
    "                \n",
    "                if np.isnan(data.at[idx, put_col]):\n",
    "                    data.at[idx, put_col] = default_iv\n",
    "    \n",
    "    # Phase 4: Smoothing and consistency\n",
    "    print(\"\\nPhase 4: Applying smoothing and consistency checks...\")\n",
    "    for idx, row in data.iterrows():\n",
    "        for strike, cols in strike_dict.items():\n",
    "            call_col = cols['call']\n",
    "            put_col = cols['put']\n",
    "            \n",
    "            avg_iv = (data.at[idx, call_col] + data.at[idx, put_col]) / 2\n",
    "            data.at[idx, call_col] = 0.9 * data.at[idx, call_col] + 0.1 * avg_iv\n",
    "            data.at[idx, put_col] = 0.9 * data.at[idx, put_col] + 0.1 * avg_iv\n",
    "    \n",
    "    # Final check for missing values\n",
    "    missing_cols = data[iv_columns].isna().any()\n",
    "    if missing_cols.any():\n",
    "        print(\"\\nWarning: Still have missing values in columns:\")\n",
    "        print(missing_cols[missing_cols].index.tolist())\n",
    "        # Use global mean as final fallback\n",
    "        global_mean = data[iv_columns].mean().mean()\n",
    "        for col in iv_columns:\n",
    "            data[col].fillna(global_mean, inplace=True)\n",
    "    \n",
    "    # Ensure all values are within reasonable bounds\n",
    "    for col in iv_columns:\n",
    "        data[col] = np.clip(data[col], 0.01, 1.0)\n",
    "    \n",
    "    print(\"\\nPrediction completed successfully\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation split...\n",
      "Training set size: 142672\n",
      "Validation set size: 35668\n",
      "\n",
      "Running validation...\n",
      "Starting simple volatility prediction...\n",
      "\n",
      "Phase 1: Applying put-call parity...\n",
      "\n",
      "Phase 2: Calculating basic statistics...\n",
      "Calculating basic statistics...\n",
      "Created 112 statistical features\n",
      "\n",
      "Phase 3: Performing simple interpolation...\n",
      "\n",
      "Phase 4: Applying smoothing and consistency checks...\n",
      "\n",
      "Prediction completed successfully\n",
      "\n",
      "Validation MSE (masked points only): 0.000000000000\n"
     ]
    }
   ],
   "source": [
    "# Create validation split\n",
    "print(\"Creating validation split...\")\n",
    "train_df, val_df = train_test_split(train, test_size=0.2, random_state=42)\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "\n",
    "# Apply to validation set\n",
    "print(\"\\nRunning validation...\")\n",
    "val_pred = predict_iv(val_df)\n",
    "\n",
    "# Calculate MSE only on originally masked validation points\n",
    "mse_vals = []\n",
    "for col in iv_columns:\n",
    "    if col in val_df.columns and col in val_pred.columns:\n",
    "        mask = val_df[col].isna() & val_pred[col].notna()\n",
    "        if mask.any():\n",
    "            se = (val_df.loc[mask, col] - val_pred.loc[mask, col]) ** 2\n",
    "            mse_vals.append(se.mean())\n",
    "\n",
    "validation_mse = np.mean(mse_vals) if mse_vals else 0\n",
    "print(f\"\\nValidation MSE (masked points only): {validation_mse:.12f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating final predictions...\n",
      "Starting simple volatility prediction...\n",
      "\n",
      "Phase 1: Applying put-call parity...\n",
      "\n",
      "Phase 2: Calculating basic statistics...\n",
      "Calculating basic statistics...\n",
      "Created 100 statistical features\n",
      "\n",
      "Phase 3: Performing simple interpolation...\n",
      "\n",
      "Phase 4: Applying smoothing and consistency checks...\n",
      "\n",
      "Phase 4: Applying smoothing and consistency checks...\n",
      "\n",
      "Prediction completed successfully\n",
      "\n",
      "Prediction completed successfully\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Missing values detected",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m submission\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m sample_sub\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Verify no missing values\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m submission\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing values detected\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m submission\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubmission.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal Submission Preview:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Missing values detected"
     ]
    }
   ],
   "source": [
    "# Apply to test set\n",
    "print(\"Generating final predictions...\")\n",
    "test_pred = predict_iv(test)\n",
    "\n",
    "# Prepare submission\n",
    "submission = test_pred[['timestamp'] + iv_columns].cosubmission.to_csv('submission.csv', index=False)dex=False)\n",
    "\n",
    "print(\"\\nFinal Submission Preview:\")\n",
    "print(submission.head())\n",
    "print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "print(f\"Validation MSE: {validation_mse:.12f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
