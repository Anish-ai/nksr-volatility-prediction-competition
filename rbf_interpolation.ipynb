{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBF Interpolation for Volatility Prediction\n",
    "\n",
    "This notebook implements a Radial Basis Function (RBF) interpolation approach for volatility prediction, with advanced feature engineering and statistical methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.interpolate import Rbf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Train data shape: (178340, 97)\n",
      "Test data shape: (12065, 96)\n",
      "Sample submission shape: (12065, 53)\n",
      "\n",
      "Number of IV columns: 52\n",
      "\n",
      "Number of unique strikes: 36\n",
      "\n",
      "Strike dictionary:\n",
      "{'24000': {'call': 'call_iv_24000', 'put': 'put_iv_24000'}, '24100': {'call': 'call_iv_24100', 'put': 'put_iv_24100'}, '24200': {'call': 'call_iv_24200', 'put': 'put_iv_24200'}, '24300': {'call': 'call_iv_24300', 'put': 'put_iv_24300'}, '24400': {'call': 'call_iv_24400', 'put': 'put_iv_24400'}, '24500': {'call': 'call_iv_24500', 'put': 'put_iv_24500'}, '24600': {'call': 'call_iv_24600', 'put': 'put_iv_24600'}, '24700': {'call': 'call_iv_24700', 'put': 'put_iv_24700'}, '24800': {'call': 'call_iv_24800', 'put': 'put_iv_24800'}, '24900': {'call': 'call_iv_24900', 'put': 'put_iv_24900'}, '25000': {'call': 'call_iv_25000', 'put': 'put_iv_25000'}, '25100': {'call': 'call_iv_25100', 'put': 'put_iv_25100'}, '25200': {'call': 'call_iv_25200', 'put': 'put_iv_25200'}, '25300': {'call': 'call_iv_25300', 'put': 'put_iv_25300'}, '25400': {'call': 'call_iv_25400', 'put': 'put_iv_25400'}, '25500': {'call': 'call_iv_25500', 'put': 'put_iv_25500'}, '25600': {'call': 'call_iv_25600', 'put': None}, '25700': {'call': 'call_iv_25700', 'put': None}, '25800': {'call': 'call_iv_25800', 'put': None}, '25900': {'call': 'call_iv_25900', 'put': None}, '26000': {'call': 'call_iv_26000', 'put': None}, '26100': {'call': 'call_iv_26100', 'put': None}, '26200': {'call': 'call_iv_26200', 'put': None}, '26300': {'call': 'call_iv_26300', 'put': None}, '26400': {'call': 'call_iv_26400', 'put': None}, '26500': {'call': 'call_iv_26500', 'put': None}, '23000': {'call': None, 'put': 'put_iv_23000'}, '23100': {'call': None, 'put': 'put_iv_23100'}, '23200': {'call': None, 'put': 'put_iv_23200'}, '23300': {'call': None, 'put': 'put_iv_23300'}, '23400': {'call': None, 'put': 'put_iv_23400'}, '23500': {'call': None, 'put': 'put_iv_23500'}, '23600': {'call': None, 'put': 'put_iv_23600'}, '23700': {'call': None, 'put': 'put_iv_23700'}, '23800': {'call': None, 'put': 'put_iv_23800'}, '23900': {'call': None, 'put': 'put_iv_23900'}}\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "train = pd.read_parquet('train_data.parquet')\n",
    "test = pd.read_parquet('test_data.parquet')\n",
    "sample_sub = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "print(f\"\\nTrain data shape: {train.shape}\")\n",
    "print(f\"Test data shape: {test.shape}\")\n",
    "print(f\"Sample submission shape: {sample_sub.shape}\")\n",
    "\n",
    "# Get all IV columns from TEST data\n",
    "iv_columns = [col for col in test.columns if col.startswith(('call_iv_', 'put_iv_'))]\n",
    "print(f\"\\nNumber of IV columns: {len(iv_columns)}\")\n",
    "\n",
    "# Create strike dictionary from TEST columns\n",
    "strike_dict = {}\n",
    "for col in iv_columns:\n",
    "    strike = col.split('_')[-1]\n",
    "    if strike not in strike_dict:\n",
    "        strike_dict[strike] = {'call': None, 'put': None}\n",
    "    \n",
    "    if col.startswith('call_iv_'):\n",
    "        strike_dict[strike]['call'] = col\n",
    "    else:\n",
    "        strike_dict[strike]['put'] = col\n",
    "\n",
    "print(f\"\\nNumber of unique strikes: {len(strike_dict)}\")\n",
    "print(\"\\nStrike dictionary:\")\n",
    "print(strike_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Statistical Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating advanced statistics...\n",
      "Created 107 statistical features\n",
      "\n",
      "Sample statistics shape: (100, 107)\n",
      "\n",
      "Statistical features:\n",
      "['timestamp', 'underlying', 'expiry', 'call_iv_23500', 'call_iv_23600', 'call_iv_23700', 'call_iv_23800', 'call_iv_23900', 'call_iv_24000', 'call_iv_24100', 'call_iv_24200', 'call_iv_24300', 'call_iv_24400', 'call_iv_24500', 'call_iv_24600', 'call_iv_24700', 'call_iv_24800', 'call_iv_24900', 'call_iv_25000', 'call_iv_25100', 'call_iv_25200', 'call_iv_25300', 'call_iv_25400', 'call_iv_25500', 'call_iv_25600', 'call_iv_25700', 'call_iv_25800', 'call_iv_25900', 'call_iv_26000', 'put_iv_22500', 'put_iv_22600', 'put_iv_22700', 'put_iv_22800', 'put_iv_22900', 'put_iv_23000', 'put_iv_23100', 'put_iv_23200', 'put_iv_23300', 'put_iv_23400', 'put_iv_23500', 'put_iv_23600', 'put_iv_23700', 'put_iv_23800', 'put_iv_23900', 'put_iv_24000', 'put_iv_24100', 'put_iv_24200', 'put_iv_24300', 'put_iv_24400', 'put_iv_24500', 'put_iv_24600', 'put_iv_24700', 'put_iv_24800', 'put_iv_24900', 'put_iv_25000', 'X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'X24', 'X25', 'X26', 'X27', 'X28', 'X29', 'X30', 'X31', 'X32', 'X33', 'X34', 'X35', 'X36', 'X37', 'X38', 'X39', 'X40', 'X41', 'underlying_returns', 'realized_vol', 'vol_of_vol', 'vol_skew', 'vol_kurt', 'vol_term_5', 'vol_term_10', 'vol_term_20', 'vol_term_60', 'vol_regime']\n"
     ]
    }
   ],
   "source": [
    "def calculate_advanced_statistics(df):\n",
    "    \"\"\"Calculate advanced statistical metrics\"\"\"\n",
    "    print(\"Calculating advanced statistics...\")\n",
    "    stats = df.copy()\n",
    "    \n",
    "    # Calculate returns first\n",
    "    stats['underlying_returns'] = np.log(stats['underlying'] / stats['underlying'].shift(1))\n",
    "    \n",
    "    # Realized volatility\n",
    "    stats['realized_vol'] = stats['underlying_returns'].rolling(window=20).std() * np.sqrt(252)\n",
    "    \n",
    "    # Volatility of volatility\n",
    "    stats['vol_of_vol'] = stats['underlying_returns'].rolling(window=20).std().rolling(window=20).std()\n",
    "    \n",
    "    # Volatility skew\n",
    "    stats['vol_skew'] = stats['underlying_returns'].rolling(window=20).skew()\n",
    "    \n",
    "    # Volatility kurtosis\n",
    "    stats['vol_kurt'] = stats['underlying_returns'].rolling(window=20).kurt()\n",
    "    \n",
    "    # Volatility term structure\n",
    "    for window in [5, 10, 20, 60]:\n",
    "        stats[f'vol_term_{window}'] = stats['underlying_returns'].rolling(window=window).std() * np.sqrt(252)\n",
    "    \n",
    "    # Volatility regime indicators - create after filling missing values\n",
    "    stats = stats.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "    try:\n",
    "        stats['vol_regime'] = pd.qcut(stats['realized_vol'], q=5, labels=['very_low', 'low', 'medium', 'high', 'very_high'])\n",
    "    except ValueError:\n",
    "        # Handle case when there are not enough unique values\n",
    "        stats['vol_regime'] = pd.Series(['medium'] * len(stats), index=stats.index)\n",
    "    \n",
    "    print(f\"Created {len(stats.columns)} statistical features\")\n",
    "    return stats\n",
    "\n",
    "# Test statistics calculation on a small sample\n",
    "sample_stats = calculate_advanced_statistics(train.head(100))\n",
    "print(f\"\\nSample statistics shape: {sample_stats.shape}\")\n",
    "print(\"\\nStatistical features:\")\n",
    "print(sample_stats.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RBF Interpolation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBF interpolation function defined successfully\n"
     ]
    }
   ],
   "source": [
    "def rbf_interpolation(x, y, x_new, function='multiquadric'):\n",
    "    \"\"\"Perform RBF interpolation\"\"\"\n",
    "    rbf = Rbf(x, y, function=function)\n",
    "    return rbf(x_new)\n",
    "\n",
    "print(\"RBF interpolation function defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_iv(data):\n",
    "    print(\"Starting RBF interpolation prediction...\")\n",
    "    data = data.copy()\n",
    "    \n",
    "    # Phase 1: Put-call parity\n",
    "    print(\"\\nPhase 1: Applying put-call parity...\")\n",
    "    for strike, cols in strike_dict.items():\n",
    "        call_col = cols['call']\n",
    "        put_col = cols['put']\n",
    "        \n",
    "        # Skip if columns don't exist in the data\n",
    "        if call_col not in data.columns or put_col not in data.columns:\n",
    "            continue\n",
    "        \n",
    "        call_mask = data[call_col].isna() & data[put_col].notna()\n",
    "        data.loc[call_mask, call_col] = data.loc[call_mask, put_col]\n",
    "        \n",
    "        put_mask = data[put_col].isna() & data[call_col].notna()\n",
    "        data.loc[put_mask, put_col] = data.loc[put_mask, call_col]\n",
    "    \n",
    "    # Phase 2: Calculate advanced statistics\n",
    "    print(\"\\nPhase 2: Calculating advanced statistics...\")\n",
    "    stats = calculate_advanced_statistics(data)\n",
    "    \n",
    "    # Phase 3: RBF interpolation\n",
    "    print(\"\\nPhase 3: Performing RBF interpolation...\")\n",
    "    for idx, row in data.iterrows():\n",
    "        # Get available IVs for this timestamp\n",
    "        available_strikes = []\n",
    "        available_ivs = []\n",
    "        \n",
    "        # Collect all available IVs\n",
    "        for s, c in strike_dict.items():\n",
    "            if c['call'] in data.columns and not np.isnan(data.at[idx, c['call']]):\n",
    "                available_strikes.append(float(s))\n",
    "                available_ivs.append(data.at[idx, c['call']])\n",
    "            if c['put'] in data.columns and not np.isnan(data.at[idx, c['put']]):\n",
    "                available_strikes.append(float(s))\n",
    "                available_ivs.append(data.at[idx, c['put']])\n",
    "        \n",
    "        # If we have enough points for interpolation\n",
    "        if len(available_strikes) >= 3:\n",
    "            # For each missing IV, interpolate using available values\n",
    "            for strike, cols in strike_dict.items():\n",
    "                strike_price = float(strike)\n",
    "                call_col = cols['call']\n",
    "                put_col = cols['put']\n",
    "                \n",
    "                # Skip if columns don't exist\n",
    "                if call_col not in data.columns or put_col not in data.columns:\n",
    "                    continue\n",
    "                \n",
    "                # Convert to log-moneyness space\n",
    "                k = np.log(np.array(available_strikes) / row['underlying'])\n",
    "                \n",
    "                try:\n",
    "                    k_pred = np.log(strike_price / row['underlying'])\n",
    "                    iv_pred = rbf_interpolation(k, available_ivs, k_pred)\n",
    "                    \n",
    "                    if np.isnan(data.at[idx, call_col]):\n",
    "                        data.at[idx, call_col] = iv_pred\n",
    "                    \n",
    "                    if np.isnan(data.at[idx, put_col]):\n",
    "                        data.at[idx, put_col] = iv_pred\n",
    "                except:\n",
    "                    # If RBF fails, use nearest available IV\n",
    "                    if np.isnan(data.at[idx, call_col]) or np.isnan(data.at[idx, put_col]):\n",
    "                        nearest_idx = np.argmin(np.abs(np.array(available_strikes) - strike_price))\n",
    "                        iv_pred = available_ivs[nearest_idx]\n",
    "                        \n",
    "                        if np.isnan(data.at[idx, call_col]):\n",
    "                            data.at[idx, call_col] = iv_pred\n",
    "                        \n",
    "                        if np.isnan(data.at[idx, put_col]):\n",
    "                            data.at[idx, put_col] = iv_pred\n",
    "        else:\n",
    "            # Not enough points for interpolation, use mean of available IVs or global mean/median\n",
    "            if available_strikes:\n",
    "                default_iv = np.mean(available_ivs)\n",
    "            else:\n",
    "                # Use statistics-based fallback\n",
    "                realized_vol = stats.at[idx, 'realized_vol']\n",
    "                default_iv = min(max(realized_vol, 0.01), 1.0)\n",
    "            \n",
    "            for strike, cols in strike_dict.items():\n",
    "                call_col = cols['call']\n",
    "                put_col = cols['put']\n",
    "                \n",
    "                if call_col in data.columns and np.isnan(data.at[idx, call_col]):\n",
    "                    data.at[idx, call_col] = default_iv\n",
    "                \n",
    "                if put_col in data.columns and np.isnan(data.at[idx, put_col]):\n",
    "                    data.at[idx, put_col] = default_iv\n",
    "    \n",
    "    # Phase 4: Smoothing and consistency\n",
    "    print(\"\\nPhase 4: Applying smoothing and consistency checks...\")\n",
    "    for idx, row in data.iterrows():\n",
    "        for strike, cols in strike_dict.items():\n",
    "            call_col = cols['call']\n",
    "            put_col = cols['put']\n",
    "            \n",
    "            # Skip if columns don't exist\n",
    "            if call_col not in data.columns or put_col not in data.columns:\n",
    "                continue\n",
    "            \n",
    "            avg_iv = (data.at[idx, call_col] + data.at[idx, put_col]) / 2\n",
    "            data.at[idx, call_col] = 0.9 * data.at[idx, call_col] + 0.1 * avg_iv\n",
    "            data.at[idx, put_col] = 0.9 * data.at[idx, put_col] + 0.1 * avg_iv\n",
    "    \n",
    "    # Ensure all values are within reasonable bounds\n",
    "    for col in iv_columns:\n",
    "        if col in data.columns:\n",
    "            data[col] = np.clip(data[col], 0.01, 1.0)\n",
    "    \n",
    "    print(\"\\nPrediction completed successfully\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation split...\n",
      "Training set size: 142672\n",
      "Validation set size: 35668\n",
      "\n",
      "Running validation...\n",
      "Starting RBF interpolation prediction...\n",
      "\n",
      "Phase 1: Applying put-call parity...\n",
      "\n",
      "Phase 2: Calculating advanced statistics...\n",
      "Calculating advanced statistics...\n",
      "Created 107 statistical features\n",
      "\n",
      "Phase 3: Performing RBF interpolation...\n",
      "Created 107 statistical features\n",
      "\n",
      "Phase 3: Performing RBF interpolation...\n",
      "\n",
      "Phase 4: Applying smoothing and consistency checks...\n",
      "\n",
      "Phase 4: Applying smoothing and consistency checks...\n",
      "\n",
      "Prediction completed successfully\n",
      "\n",
      "Validation MSE (masked points only): 0.000000000000\n",
      "\n",
      "Prediction completed successfully\n",
      "\n",
      "Validation MSE (masked points only): 0.000000000000\n"
     ]
    }
   ],
   "source": [
    "# Create validation split\n",
    "print(\"Creating validation split...\")\n",
    "train_df, val_df = train_test_split(train, test_size=0.2, random_state=42)\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "\n",
    "# Apply to validation set\n",
    "print(\"\\nRunning validation...\")\n",
    "val_pred = predict_iv(val_df)\n",
    "\n",
    "# Calculate MSE only on originally masked validation points\n",
    "mse_vals = []\n",
    "for col in iv_columns:\n",
    "    if col in val_df.columns and col in val_pred.columns:\n",
    "        mask = val_df[col].isna() & val_pred[col].notna()\n",
    "        if mask.any():\n",
    "            se = (val_df.loc[mask, col] - val_pred.loc[mask, col]) ** 2\n",
    "            mse_vals.append(se.mean())\n",
    "\n",
    "validation_mse = np.mean(mse_vals) if mse_vals else 0\n",
    "print(f\"\\nValidation MSE (masked points only): {validation_mse:.12f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating final predictions...\n",
      "Starting RBF interpolation prediction...\n",
      "\n",
      "Phase 1: Applying put-call parity...\n",
      "\n",
      "Phase 2: Calculating advanced statistics...\n",
      "Calculating advanced statistics...\n",
      "Created 106 statistical features\n",
      "\n",
      "Phase 3: Performing RBF interpolation...\n"
     ]
    }
   ],
   "source": [
    "# Apply to test set\n",
    "print(\"Generating final predictions...\")\n",
    "test_pred = predict_iv(test)\n",
    "\n",
    "# Prepare submission\n",
    "submission = test_pred[['timestamp'] + iv_columns].copy()\n",
    "submission.columns = sample_sub.columns\n",
    "\n",
    "# Verify no missing values\n",
    "assert submission.isna().sum().sum() == 0, \"Missing values detected\"\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\nFinal Submission Preview:\")\n",
    "print(submission.head())\n",
    "print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "print(f\"Validation MSE: {validation_mse:.12f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
